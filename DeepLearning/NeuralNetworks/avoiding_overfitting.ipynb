{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Worrying-About-Overfitting\" data-toc-modified-id=\"Worrying-About-Overfitting-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Worrying About Overfitting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Use-Train-Validation-Test\" data-toc-modified-id=\"Use-Train-Validation-Test-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Use Train-Validation-Test</a></span></li><li><span><a href=\"#Model-Complexity-Graph\" data-toc-modified-id=\"Model-Complexity-Graph-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Model Complexity Graph</a></span><ul class=\"toc-item\"><li><span><a href=\"#Early-Stopping\" data-toc-modified-id=\"Early-Stopping-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Early Stopping</a></span></li></ul></li></ul></li><li><span><a href=\"#When-a-Good-Model-Goes-Bad\" data-toc-modified-id=\"When-a-Good-Model-Goes-Bad-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>When a Good Model Goes Bad</a></span><ul class=\"toc-item\"><li><span><a href=\"#L1-Regularization---Absolute-Value\" data-toc-modified-id=\"L1-Regularization---Absolute-Value-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>L1 Regularization - Absolute Value</a></span></li><li><span><a href=\"#L2-Regularization---Squared-Value\" data-toc-modified-id=\"L2-Regularization---Squared-Value-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>L2 Regularization - Squared Value</a></span></li><li><span><a href=\"#Comparing-L1-&amp;-L2-Regularization\" data-toc-modified-id=\"Comparing-L1-&amp;-L2-Regularization-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Comparing L1 &amp; L2 Regularization</a></span></li><li><span><a href=\"#Code-Implementation\" data-toc-modified-id=\"Code-Implementation-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Code Implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overcomplicated-Model\" data-toc-modified-id=\"Overcomplicated-Model-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Overcomplicated Model</a></span></li><li><span><a href=\"#Regulated-Model\" data-toc-modified-id=\"Regulated-Model-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Regulated Model</a></span></li></ul></li></ul></li><li><span><a href=\"#Dropout\" data-toc-modified-id=\"Dropout-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Dropout</a></span><ul class=\"toc-item\"><li><span><a href=\"#Avoiding-the-Self-Perpetuating-Strength-Training\" data-toc-modified-id=\"Avoiding-the-Self-Perpetuating-Strength-Training-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Avoiding the Self-Perpetuating Strength Training</a></span></li><li><span><a href=\"#Example-Code\" data-toc-modified-id=\"Example-Code-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Example Code</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Worrying About Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A big issue is making sure we don't overfit our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Use Train-Validation-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Think of **training** as what you study for a test\n",
    "- Think of **validation** is using a practice test (note sometimes called **dev**)\n",
    "- Think of **testing** as what you use to judge the model \n",
    "\n",
    "> ***holdout*** is when your test dataset is never used for training (unlike in cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> The **validation** & **test** sets should come from the same distribution.\n",
    ">\n",
    "> _Why would this matter?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Model Complexity Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Underfitting\n",
    "    + low complexity --> high bias, low variance\n",
    "    + training error: large\n",
    "    + testing error: large\n",
    "- Overfitting\n",
    "    + high complexity --> low bias, high variance\n",
    "    + training error: low\n",
    "    + testing error: large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_error = np.array([5,3.5,2,3,4])\n",
    "train_error = np.array([4.5,3,1.5,1,0.5])\n",
    "n_epochs = np.array([5,50,100,200,300])\n",
    "\n",
    "plt.scatter(n_epochs, train_error,)\n",
    "plt.scatter(n_epochs, validation_error)\n",
    "plt.legend(['train error','validation error'])\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Early Stopping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's first create a model we can play around with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get data to train with\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target # Note targets are simply 0-9 associated with class\n",
    "\n",
    "# Convert target to one-hot encoded vector\n",
    "y = keras.utils.to_categorical(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **NOTE**:\n",
    ">\n",
    "> We could have kept the targets as integers instead of using `to_categorical()` to make\n",
    "> one-hot encoded vectors. In that case we would use [`SparseCategoricalCrossentropy`](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\n",
    ">\n",
    "> For more on Keras' different built-in losses, see the documentation: https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, y, random_state=27, test_size=0.2)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid =\\\n",
    "    train_test_split(X_train, y_train, random_state=27, test_size=0.2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu', input_dim=64))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "# Note we use 'categorical_crossentropy' since target is one-hot encoded\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We train our model but only keep the best model it comes across. We can do this with a [ModelCheckpoint callback](https://keras.io/callbacks/#modelcheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\"best_model.h5\",\n",
    "                                             save_best_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics = ['loss','val_loss']\n",
    "for metric in metrics:\n",
    "    plt.plot(history.history[metric], label=metric)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now points to the best model found during the fit\n",
    "model = keras.models.load_model(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also stop our training early when our test error isn't really changing. We can do this with a [EarlyStopping callback](https://keras.io/callbacks/#earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recreating/resetting the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu', input_dim=64))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "checkpoint = keras.callbacks.EarlyStopping(\n",
    "                                monitor='val_loss', # What to watch\n",
    "                                min_delta=0.1, # How much change to get\n",
    "                                patience=5 # No change after 5 epochs\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics = ['loss','val_loss']\n",
    "for metric in metrics:\n",
    "    plt.plot(history.history[metric], label=metric)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# When a Good Model Goes Bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When a model has large weights, the model is \"too confident\"\n",
    "\n",
    "We need to punish large (confident) weights by contributing them to the error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/punishing_model_metaphor.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## L1 Regularization - Absolute Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Tend to get sparse vectors (small weights go to 0)\n",
    "- Reduce number of weights\n",
    "- Good feature selection to pick out importance\n",
    "\n",
    "$$ J(W,b) = -\\dfrac{1}{m} \\sum^m_{i=1}\\big[\\mathcal{L}(\\hat y_i, y_i)+ \\dfrac{\\lambda}{m}|w_i| \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## L2 Regularization - Squared Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Not sparse vectors (weights homogeneous & small)\n",
    "- Tends to give better results for training\n",
    "\n",
    "    \n",
    "$$ J(W,b) = -\\dfrac{1}{m} \\sum^m_{i=1}\\big[\\mathcal{L}(\\hat y_i, y_i)+ \\dfrac{\\lambda}{m}w_i^2 \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Comparing L1 & L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Typically you'll want to use L2 regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "+ subtle; consider vectors: [1,0] & [0.5, 0.5] \n",
    "+ recall we want smallest value for our value\n",
    "+ L2 prefers [0.5,0.5] over [1,0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Overcomplicated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_complex_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim=64))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_complex_model()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics = ['loss','val_loss']\n",
    "for metric in metrics:\n",
    "    plt.plot(history.history[metric], label=metric)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can see our overcomplicated model could use some regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Regulated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_regulated_model():\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(\n",
    "            32, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l2(l2=0.01),\n",
    "            input_dim=64)\n",
    "    )\n",
    "    model.add(\n",
    "        Dense(\n",
    "            24, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l2(l2=0.01)\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        Dense(\n",
    "            24, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l2(l2=0.01)\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_regulated_model()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics = ['loss','val_loss']\n",
    "for metric in metrics:\n",
    "    plt.plot(history.history[metric], label=metric)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You want to even out your workouts, otherwise you may have some strange results..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='images/homer-dropout-comparison.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Well, our neural network models are the same way. The model should get _evenly_ trained. We don't want to train the same node/pathway over and over again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Avoiding the Self-Perpetuating Strength Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When working out, we'd train our left and right arms evenly and switch our exercise routine throughout the week.\n",
    "\n",
    "In neural networks, we switch around which nodes we use during our training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assign a probability of using a given node for that epoch (usually about 20% chance). When we have many epochs, we likely will even out the randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='images/layered-neural-net.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Dense(32, input_dim=64, activation='relu', name='input_layer'))\n",
    "model.add(Dropout(0.2, name='input_dropout'))\n",
    "# Hidden Layer\n",
    "model.add(Dense(24, activation='relu', name='hidden_layer1'))\n",
    "model.add(Dropout(0.2, name='hidden_layer1_dropout'))\n",
    "# Hidden Layer\n",
    "model.add(Dense(24, activation='relu', name='hidden_layer2'))\n",
    "model.add(Dropout(0.2, name='hidden_layer2_dropout'))\n",
    "# Output Layer\n",
    "model.add(Dense(n_classes, activation='softmax', name='output'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics = ['loss','val_loss']\n",
    "for metric in metrics:\n",
    "    plt.plot(history.history[metric], label=metric)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "271px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
